{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f39ae80",
   "metadata": {},
   "source": [
    "# üìö Week 2: Machine Learning Foundations ‚Äì Notes\n",
    "\n",
    "## üîπ Day 8: ML Basics\n",
    "- **Key Concepts**\n",
    "  - **Features (X):** Input variables used for prediction.\n",
    "  - **Labels (y):** Target/output variable.\n",
    "  - **Overfitting:** Model fits training data too well, performs poorly on unseen data.\n",
    "  - **Underfitting:** Model is too simple, misses important patterns.\n",
    "- **General Workflow**\n",
    "  1. Define problem\n",
    "  2. Prepare data\n",
    "  3. Choose model\n",
    "  4. Train\n",
    "  5. Evaluate\n",
    "  6. Improve\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Day 9: Train/Test Split\n",
    "- **Purpose:** Evaluate model performance on unseen data.\n",
    "- **Sklearn Syntax**\n",
    "  ```python\n",
    "  from sklearn.model_selection import train_test_split\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944c1df8",
   "metadata": {},
   "source": [
    "- Baseline Models\n",
    "\n",
    "    - Regression: Predict mean of target.\n",
    "    - Classification: Predict majority class.\n",
    "\n",
    "## Day 10: Linear Regression\n",
    "\n",
    "- Use Case: Predict continuous values (e.g., housing prices).\n",
    "- Formula: ≈∑ = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + Œ≤‚ÇÇx‚ÇÇ + ... + Œ≤‚Çôx‚Çô\n",
    "\n",
    "- Evaluation Metrics\n",
    "    - R¬≤ (explained variance)\n",
    "    - MAE (Mean Absolute Error)\n",
    "    - MSE (Mean Squared Error)\n",
    "\n",
    "- Visualization Idea: Plot predicted vs actual values.\n",
    "\n",
    "## Day 11: Logistic Regression\n",
    "\n",
    "- Use Case: Binary classification (e.g., Titanic survival).\n",
    "- Output: Probability between 0 and 1 ‚Üí mapped to class (0/1).\n",
    "\n",
    "- Interpretation:\n",
    "    - coef_ shows effect of each feature on odds of outcome.\n",
    "    - Positive coef ‚Üí increases probability of class 1.\n",
    "    - Evaluation: Confusion matrix + metrics (see Day 12).\n",
    "\n",
    "## Day 12: Evaluation Metrics\n",
    "\n",
    "- Classification Metrics\n",
    "    - Accuracy = (TP + TN) / Total\n",
    "    - Precision = TP / (TP + FP)\n",
    "    - Recall = TP / (TP + FN)\n",
    "    - F1 = 2 √ó (Precision √ó Recall) / (Precision + Recall)\n",
    "    - ROC-AUC: Probability that model ranks a random positive higher than random negative.\n",
    "\n",
    "<strong>Tip: Always compare multiple metrics, not just accuracy.</strong><br>\n",
    "<strong>Custom Function Example</strong>\n",
    "\n",
    "<pre>\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "def evaluate_classification(y_true, y_pred, y_proba=None):\n",
    "    metrics = {\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Precision\": precision_score(y_true, y_pred),\n",
    "        \"Recall\": recall_score(y_true, y_pred),\n",
    "        \"F1\": f1_score(y_true, y_pred)\n",
    "    }\n",
    "    if y_proba is not None:\n",
    "        metrics[\"ROC-AUC\"] = roc_auc_score(y_true, y_proba)\n",
    "    return metrics\n",
    "</pre>\n",
    "\n",
    "## 13: Project ‚Äì Titanic Survival Prediction\n",
    "\n",
    "- Steps\n",
    "\n",
    "    1. Load dataset\n",
    "\n",
    "    2. Clean data (handle missing values, drop irrelevant cols)\n",
    "\n",
    "    3. Feature engineering (e.g., convert categorical ‚Üí numeric)\n",
    "\n",
    "    4. Train Logistic Regression model\n",
    "\n",
    "    5. Evaluate performance (accuracy, precision, recall, F1, ROC-AUC)\n",
    "\n",
    "    6. Interpret results (important features)\n",
    "\n",
    "- Deliverable: Clear explanation + visualizations."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
